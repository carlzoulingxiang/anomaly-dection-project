{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0977328-e447-4e1d-a359-8ac9b28b6991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "870842f1-d0d6-4983-8bbf-14142743457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv('../data/BGL_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8ee274-a0d0-44c9-aa95-7587b31c1924",
   "metadata": {},
   "source": [
    "## Split Data to Training Set and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01a0687b-9b81-4425-85e3-1c44ff6c67c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X = df['info']\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d34436-8dbd-4d15-82bd-709e1268d39b",
   "metadata": {},
   "source": [
    "## Word of Bags Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e45d9dc9-add2-4946-82f3-58abfd13aa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "train_count = vectorizer.fit_transform(X_train)\n",
    "test_count = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a199af8a-a41f-456d-8680-e5afc9ae2aa5",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "120e8cbe-83ca-484a-a866-9ef63fc4eea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 17.87837505340576 seconds ###\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "logReg = LogisticRegression(max_iter=300)\n",
    "logReg.fit(train_count, y_train)\n",
    "\n",
    "print(\"### %s seconds ###\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72ce83c1-83ce-4dad-a0d6-db57dab81db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss on test set is: 9.917162196816884e-05\n",
      "Accuracy Score on test set is: 0.9999891500534839\n"
     ]
    }
   ],
   "source": [
    "pred_test = logReg.predict(test_count)\n",
    "pred_prob = logReg.predict_proba(test_count)\n",
    "print(\"Log loss on test set is:\",metrics.log_loss(y_test, pred_prob))\n",
    "print(\"Accuracy Score on test set is:\",metrics.accuracy_score(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2def608-4874-43f3-a325-053666583a37",
   "metadata": {},
   "source": [
    "Logistic regression was the first model that we constructed. Each label was considered independent in this approach, therefore, we fed as input the converted comments by word of bags and use sklearn's default LogisticRegression package trained a model on training data. The total training time for training model around 18s. Predictions on test were comparable but in order to solve overfitting, there was still scope for improvement. A test accuracy of 0.999989 was obtained and log loss on test set is 0.00009917."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8121c72f-5fd3-424a-b6f9-fcb1a214d9eb",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d8cc093-2711-4db7-aad0-94336275702f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When alpha value is  0.01 , Log loss on test set is: 0.0168354131474122\n",
      "When alpha value is  0.01 , Accuracy Score on test set is: 0.9987809765973036\n",
      "When alpha value is  0.015 , Log loss on test set is: 0.016980711594519065\n",
      "When alpha value is  0.015 , Accuracy Score on test set is: 0.9987809765973036\n",
      "When alpha value is  0.1 , Log loss on test set is: 0.017714921099963005\n",
      "When alpha value is  0.1 , Accuracy Score on test set is: 0.9986960917216184\n",
      "When alpha value is  0.5 , Log loss on test set is: 0.01844655275583625\n",
      "When alpha value is  0.5 , Accuracy Score on test set is: 0.9986794976857702\n",
      "When alpha value is  1 , Log loss on test set is: 0.01882955439098886\n",
      "When alpha value is  1 , Accuracy Score on test set is: 0.9986539683998499\n",
      "### 16.163208770751954 seconds ###\n"
     ]
    }
   ],
   "source": [
    "#Tuning parameter alpha\n",
    "alpha = [0.01, 0.015, 0.1, 0.5, 1]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "for i in alpha:\n",
    "    nb = MultinomialNB(alpha = i)\n",
    "    nb.fit(train_count, y_train)\n",
    "    pred_test = nb.predict(test_count)\n",
    "    pred_prob = nb.predict_proba(test_count)\n",
    "    print(\"When alpha value is \", i,\", Log loss on test set is:\",metrics.log_loss(y_test, pred_prob))\n",
    "    print(\"When alpha value is \", i,\", Accuracy Score on test set is:\",metrics.accuracy_score(y_test, pred_test))\n",
    "\n",
    "    \n",
    "print(\"### %s seconds ###\" % str((time.time() - start_time)/len(alpha)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f5a084-80e7-4ac6-a5e6-05ac7d8307c6",
   "metadata": {},
   "source": [
    "Our second model is Naive Bayes. To do this, we fed as input the converted comments by word of bags into sklearn's naive_bayes package. The approximately average training time for each model is 16s. We tune the alpha parameter and found there is not much difference between each alpha value. Finally achieved an accuracy score 0.99878 on test set and the log loss is 0.016835."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76934f9b-eb6b-4f35-aa9f-203076191c0c",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36685393-c45e-4b60-8dd8-36c7a2276bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 90.76089096069336 seconds ###\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(train_count, y_train)\n",
    "\n",
    "print(\"### %s seconds ###\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a25da159-eeb2-4c7c-b461-395c379083fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss on test set is: 9.886666332440855e-06\n",
      "Accuracy Score on test set is: 0.999995532374964\n"
     ]
    }
   ],
   "source": [
    "pred_test = clf.predict(test_count)\n",
    "pred_prob = clf.predict_proba(test_count)\n",
    "print(\"Log loss on test set is:\",metrics.log_loss(y_test, pred_prob))\n",
    "print(\"Accuracy Score on test set is:\",metrics.accuracy_score(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519bc5ef-c1b9-4a72-8efc-6141ba22e229",
   "metadata": {},
   "source": [
    "The third model is random froest, which is a classification algorithm consisting of many decisions trees. The total training time is around 92s, which is much longer than logistic and naive bayes because it uses bagging and feature randomness when building each individual tree. Random forest is robust to overfitting. The log loss on test set is 0.0000098448 and the accuracy score on test set is 0.99999."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4d0cda-8896-4f09-bdd4-080800205be6",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11be0e6-ebad-40d0-a6f6-8ba8a785cca5",
   "metadata": {},
   "source": [
    "| Model               | Training Time | Log Loss     | Accuracy Score |\n",
    "|---------------------|---------------|--------------|----------------|\n",
    "| Logistic Regression | 18s           | 0.00009917   | 0.999989       |\n",
    "| Naive Bayes         | 16s           | 0.016835     | 0.998781       |\n",
    "| Random Forest          | 92s           | 0.0000098448 | 0.999995       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffca954-f521-464f-bd25-c017598895c2",
   "metadata": {},
   "source": [
    "Among the three models, we can see that Random Forest has the best performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
